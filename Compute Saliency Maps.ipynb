{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c91e0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data.utils import get_dataset\n",
    "import captum\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import DeepLift\n",
    "from captum.attr import NoiseTunnel\n",
    "\n",
    "\n",
    "from DebiAN.models.simple_cls import get_simple_classifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdb24f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6370f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "def overlay(input, cam, alpha=0.9, colormap=\"jet\"):\n",
    "    # inspired by https://github.com/frgfm/torch-cam/blob/master/torchcam/utils.py\n",
    "\n",
    "    img = transforms.ToPILImage()(input)\n",
    "    # normalize to 0,1\n",
    "    cam -= torch.min(cam)\n",
    "    cam /= torch.max(cam)\n",
    "    cam_img = transforms.ToPILImage(mode='F')(cam)\n",
    "\n",
    "    if type(colormap) is str:\n",
    "        cmap = cm.get_cmap(colormap)\n",
    "    else:\n",
    "        cmap = colormap\n",
    "\n",
    "    # Resize mask and apply colormap\n",
    "    overlay_raw = cam_img.resize(img.size, resample=Image.BILINEAR)\n",
    "    overlay_raw = cam_img.resize(img.size, resample=Image.NEAREST)\n",
    "    overlay = overlay_raw\n",
    "    overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n",
    "    # Overlay the image with the mask\n",
    "    overlayed_img = Image.fromarray((alpha * np.asarray(img) + (1 - alpha) * overlay).astype(np.uint8))\n",
    "    return overlayed_img\n",
    "\n",
    "\n",
    "def attribute_image_features(net,algorithm, input,label, **kwargs):\n",
    "    net.zero_grad()\n",
    "    tensor_attributions = algorithm.attribute(input,\n",
    "                                              target=label,\n",
    "                                              **kwargs\n",
    "                                             )\n",
    "    return tensor_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc840258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed everything\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed_all(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d4954",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "326d9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  ='data/'\n",
    "result_path  ='npz/cmnist/'\n",
    "split='test'\n",
    "percent='5pct'\n",
    "_last_hidden_layer=100\n",
    "\n",
    "model_path =\"DebiAN/exp/cmnist/MLP_{}/debian_bs_256_wd_1E-04_lr_1E-03_cmnist_{}/last.pth\".format(_last_hidden_layer,percent)\n",
    "print(model_path)\n",
    "\n",
    "\n",
    "\n",
    "Saliency_methods=['integrated_gradient','smoothgrad','deeplift']\n",
    "Saliency_methods=['smoothgrad','deeplift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1719917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'npz/cmnist/' created successfully\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(result_path, exist_ok = True)\n",
    "print(\"Directory '%s' created successfully\" %result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e0f7d8",
   "metadata": {},
   "source": [
    "# Load Data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f4a38cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (feature): Sequential(\n",
      "    (0): Linear(in_features=2352, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (classifier): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset=get_dataset('cmnist',\n",
    "        data_dir=data_path,\n",
    "        dataset_split=split,\n",
    "        transform_split=\"valid\",\n",
    "        percent='1pct')\n",
    "\n",
    "if dataset.__len__()==0:\n",
    "    print('Error - data not loaded')\n",
    "\n",
    "model=get_simple_classifier('mlp',last_hidden_layer=_last_hidden_layer)\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict['model'])\n",
    "model = model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451da1f8",
   "metadata": {},
   "source": [
    "# Extract Saliency map for a specific method \n",
    "## methods in [Integrated Gradient,SmoothGrad,Deeplift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoothgrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2307/10000 [10:12<33:37,  3.81it/s]"
     ]
    }
   ],
   "source": [
    "for method in Saliency_methods:\n",
    "    print (method)\n",
    "    \n",
    "    \n",
    "    align_saliencies_maps=[]\n",
    "    conflict_saliencies_maps=[]\n",
    "    \n",
    "    for idx_img in tqdm(range(len(dataset))):\n",
    "        \n",
    "        \n",
    "        image,label,idx = dataset[idx_img]\n",
    "        l_target,l_bias=label\n",
    "        l_target=l_target.item()\n",
    "        l_bias=l_bias.item()\n",
    "        correct=0\n",
    "        bias=0\n",
    "        x=torch.unsqueeze(image, 0)\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        logits=model(x)\n",
    "        pred = logits.data.max(1, keepdim=True)[1].squeeze(1)\n",
    "        pred=pred.item()\n",
    "        \n",
    "    \n",
    "        if method=='integrated_gradient':\n",
    "            # Integrated Gradient\n",
    "            ig = IntegratedGradients(model)\n",
    "            attr_ig, delta = attribute_image_features(model,ig, x,l_target, baselines=x * 0, return_convergence_delta=True)\n",
    "            \n",
    "            saliency_map= np.transpose(attr_ig.squeeze().cpu().detach().numpy(), (1, 2, 0)).sum(2)\n",
    "\n",
    "        if method=='smoothgrad':\n",
    "            # Integrated Gradient with Smoothgrad\n",
    "            ig = IntegratedGradients(model)\n",
    "            nt = NoiseTunnel(ig)\n",
    "            attr_ig_nt = attribute_image_features(model,nt,x,l_target, baselines=x * 0, nt_type='smoothgrad_sq',\n",
    "                                                  nt_samples=100, stdevs=0.2)\n",
    "            saliency_map= np.transpose(attr_ig_nt.squeeze(0).cpu().detach().numpy(), (1, 2, 0)).sum(2)\n",
    "\n",
    "\n",
    "        if method=='deeplift':\n",
    "            #DeepLift\n",
    "            dl = DeepLift(model)\n",
    "            attr_dl = attribute_image_features(model,dl, x,l_target,baselines=x * 0)\n",
    "            saliency_map= np.transpose(attr_dl.squeeze(0).cpu().detach().numpy(), (1, 2, 0)).sum(2)\n",
    "        \n",
    "        \n",
    "        logits=model(x)\n",
    "        pred = logits.data.max(1, keepdim=True)[1].squeeze(1)\n",
    "        pred=pred.item()\n",
    "\n",
    "\n",
    "        if l_bias==l_target:\n",
    "            align_saliencies_maps.append(saliency_map)\n",
    "        else:\n",
    "            conflict_saliencies_maps.append(saliency_map)\n",
    "\n",
    "    \n",
    "    #Convert the list of maps into one tensor\n",
    "    \n",
    "    print(\"align_data {} samples for method {}\".format(len(align_saliencies_maps),method))\n",
    "    print(\"conflict_data {} samples for method {}\".format(len(conflict_saliencies_maps),method)) \n",
    "    print(\"saving saliencies to npz for method {}\".format(method))\n",
    "    npz_name_allign = 'align_'+method + \"_mlp{}_{}_cmnist_{}\".format(split,_last_hidden_layer,percent)\n",
    "    np.savez(path.join(result_path, npz_name_allign), align_saliencies_maps)\n",
    "    npz_name_conflict = 'conflict_'+method + \"_mlp{}_{}_cmnist_{}\".format(split,_last_hidden_layer,percent)\n",
    "    np.savez(path.join(result_path, npz_name_conflict), conflict_saliencies_maps)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1126b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
